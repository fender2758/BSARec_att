2025-03-05 19:12:23,277 - Namespace(data_dir='./data/', output_dir='output/', data_name='LastFM', do_eval=False, load_model=None, train_name='BSARec_att_LastFM', num_items=10, num_users=1091, lr=0.001, batch_size=256, epochs=200, no_cuda=False, log_freq=1, patience=10, num_workers=4, seed=42, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, gpu_id='0', variance=5, model_type='BSARec_att', max_seq_length=50, hidden_size=64, num_hidden_layers=2, hidden_act='gelu', num_attention_heads=2, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5, initializer_range=0.02, c=3, alpha=0.5, cuda_condition=False, data_file='./data/LastFM.txt', item_size=3647, checkpoint_path='output/BSARec_att_LastFM.pt', same_target_path='./data/LastFM_same_target.npy')
2025-03-05 19:12:23,329 - BSARecAttModel(
  (item_embeddings): Embedding(3647, 64, padding_idx=0)
  (position_embeddings): Embedding(50, 64)
  (LayerNorm): LayerNorm()
  (dropout): Dropout(p=0.5, inplace=False)
  (item_encoder): BSARecEncoder(
    (blocks): ModuleList(
      (0-1): 2 x BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer_low): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
          (attention_layer_high): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
)
2025-03-05 19:12:25,997 - Total Parameters: 370624
2025-03-05 19:13:12,633 - {'epoch': 0, 'rec_loss': '7.9362'}
2025-03-05 19:17:31,269 - Namespace(data_dir='./data/', output_dir='output/', data_name='LastFM', do_eval=False, load_model=None, train_name='BSARec_att_LastFM', num_items=10, num_users=1091, lr=0.001, batch_size=256, epochs=200, no_cuda=False, log_freq=1, patience=10, num_workers=4, seed=42, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, gpu_id='0', variance=5, model_type='BSARec_att', max_seq_length=50, hidden_size=64, num_hidden_layers=2, hidden_act='gelu', num_attention_heads=2, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5, initializer_range=0.02, c=3, alpha=0.5, cuda_condition=False, data_file='./data/LastFM.txt', item_size=3647, checkpoint_path='output/BSARec_att_LastFM.pt', same_target_path='./data/LastFM_same_target.npy')
2025-03-05 19:17:31,286 - BSARecAttModel(
  (item_embeddings): Embedding(3647, 64, padding_idx=0)
  (position_embeddings): Embedding(50, 64)
  (LayerNorm): LayerNorm()
  (dropout): Dropout(p=0.5, inplace=False)
  (item_encoder): BSARecEncoder(
    (blocks): ModuleList(
      (0-1): 2 x BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer_low): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
          (attention_layer_high): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
)
2025-03-05 19:17:33,347 - Total Parameters: 370624
2025-03-05 19:23:16,745 - Namespace(data_dir='./data/', output_dir='output/', data_name='LastFM', do_eval=False, load_model=None, train_name='BSARec_att_LastFM', num_items=10, num_users=1091, lr=0.001, batch_size=256, epochs=200, no_cuda=False, log_freq=1, patience=10, num_workers=4, seed=42, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, gpu_id='0', variance=5, model_type='BSARec_att', max_seq_length=50, hidden_size=64, num_hidden_layers=2, hidden_act='gelu', num_attention_heads=2, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5, initializer_range=0.02, c=3, alpha=0.5, cuda_condition=False, data_file='./data/LastFM.txt', item_size=3647, checkpoint_path='output/BSARec_att_LastFM.pt', same_target_path='./data/LastFM_same_target.npy')
2025-03-05 19:23:16,762 - BSARecAttModel(
  (item_embeddings): Embedding(3647, 64, padding_idx=0)
  (position_embeddings): Embedding(50, 64)
  (LayerNorm): LayerNorm()
  (dropout): Dropout(p=0.5, inplace=False)
  (item_encoder): BSARecEncoder(
    (blocks): ModuleList(
      (0): BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer_low): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
          (attention_layer_high): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer_low): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
          (attention_layer_high): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
)
2025-03-05 19:23:16,771 - Total Parameters: 370624
2025-03-05 19:25:04,033 - Namespace(data_dir='./data/', output_dir='output/', data_name='LastFM', do_eval=False, load_model=None, train_name='BSARec_att_LastFM', num_items=10, num_users=1091, lr=0.001, batch_size=256, epochs=200, no_cuda=False, log_freq=1, patience=10, num_workers=4, seed=42, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, gpu_id='0', variance=5, model_type='BSARec_att', max_seq_length=50, hidden_size=64, num_hidden_layers=2, hidden_act='gelu', num_attention_heads=2, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5, initializer_range=0.02, c=3, alpha=0.5, cuda_condition=False, data_file='./data/LastFM.txt', item_size=3647, checkpoint_path='output/BSARec_att_LastFM.pt', same_target_path='./data/LastFM_same_target.npy')
2025-03-05 19:25:04,046 - BSARecAttModel(
  (item_embeddings): Embedding(3647, 64, padding_idx=0)
  (position_embeddings): Embedding(50, 64)
  (LayerNorm): LayerNorm()
  (dropout): Dropout(p=0.5, inplace=False)
  (item_encoder): BSARecEncoder(
    (blocks): ModuleList(
      (0): BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer_low): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
          (attention_layer_high): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer_low): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
          (attention_layer_high): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
)
2025-03-05 19:25:04,054 - Total Parameters: 370624
2025-03-05 19:27:31,967 - {'epoch': 0, 'rec_loss': '7.9354'}
2025-03-05 19:27:33,602 - {'Epoch': 0, 'HR@5': '0.0211', 'NDCG@5': '0.0142', 'HR@10': '0.0349', 'NDCG@10': '0.0186', 'HR@20': '0.0587', 'NDCG@20': '0.0246'}
2025-03-05 19:27:33,611 - Validation score increased.  Saving model ...
2025-03-05 19:29:43,881 - {'epoch': 1, 'rec_loss': '7.5905'}
2025-03-05 19:29:45,806 - {'Epoch': 1, 'HR@5': '0.0257', 'NDCG@5': '0.0171', 'HR@10': '0.0413', 'NDCG@10': '0.0221', 'HR@20': '0.0716', 'NDCG@20': '0.0297'}
2025-03-05 19:29:45,816 - Validation score increased.  Saving model ...
2025-03-05 19:36:44,360 - Namespace(adam_beta1=0.9, adam_beta2=0.999, alpha=0.5, attention_probs_dropout_prob=0.5, batch_size=256, c=3, checkpoint_path='output/BSARec_att_LastFM.pt', cuda_condition=False, data_dir='./data/', data_file='./data/LastFM.txt', data_name='LastFM', do_eval=False, epochs=200, gpu_id='0', hidden_act='gelu', hidden_dropout_prob=0.5, hidden_size=64, initializer_range=0.02, item_size=3647, load_model=None, log_freq=1, lr=0.001, max_seq_length=50, model_type='BSARec_att', no_cuda=False, num_attention_heads=2, num_hidden_layers=2, num_items=10, num_users=1091, num_workers=4, output_dir='output/', patience=10, same_target_path='./data/LastFM_same_target.npy', seed=42, train_name='BSARec_att_LastFM', variance=5, weight_decay=0.0)
2025-03-05 19:36:44,401 - BSARecAttModel(
  (item_embeddings): Embedding(3647, 64, padding_idx=0)
  (position_embeddings): Embedding(50, 64)
  (LayerNorm): LayerNorm()
  (dropout): Dropout(p=0.5, inplace=False)
  (item_encoder): BSARecEncoder(
    (blocks): ModuleList(
      (0): BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer_low): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
          (attention_layer_high): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer_low): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
          (attention_layer_high): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
)
2025-03-05 19:36:44,409 - Total Parameters: 370624
